<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Home | Pradeep Bolleddu</title>
  <link rel="stylesheet" href="styles.css" />
  <style>
    /* General Tabs styling */
    .tabs__tab {
      display: none;
      width: 100vw;
      min-height: 100vh;
      padding: 20px;
      box-sizing: border-box;
      overflow-y: auto;
    }
    .tabs__tab.active {
      display: block;
    }

    /* Navigation styling */
    nav ul {
      display: flex;
      justify-content: center;
      list-style: none;
      padding: 0;
      margin: 0;
    }
    nav ul li {
      margin: 0 8px; /* reduced gap between tabs */
    }
    nav ul li a {
      text-decoration: none;
      font-size: 18px;
      color: #333;
      padding: 8px 12px;
      transition: border-bottom 0.3s;
    }
    /* Underline effect for active tab */
    nav ul li a.active {
      border-bottom: 2px solid #333;
    }

    /* Home section styling: image on the left, text on the right */
    #home-section {
      display: flex;
      align-items: center;
      gap: 20px;
      padding: 20px;
    }
    #home-section img {
      width: 180px; /* increased image size */
      height: auto;
      border-radius: 50%;
      border: 3px solid white;
    }
    #home-section .welcome-text {
      max-width: 600px;
    }
    #home-section h1 {
      margin-top: 0;
    }

    /* Projects and Publications list styling */
    #projects-list,
    #publications-list {
      list-style: none;
      padding: 0;
      text-align: left; /* ensure items start from the left */
    }
    #projects-list li,
    #publications-list li {
      margin-bottom: 15px;
    }
    /* Styling for project/publication links to appear as plain text */
    .project-link,
    .publication-link {
      font-size: 24px;
      color: inherit;
      text-decoration: none;
      cursor: pointer;
    }
    
    /* Project/Publication description styling */
    #project-content p,
    #publication-content p {
      font-family: 'Georgia', serif;
      font-size: 18px;
    }

    /* Footer styling */
    .footer {
      text-align: center;
      padding: 20px;
    }
    .footer a {
      margin: 0 10px;
    }
    .footer img {
      width: 30px;
      height: auto;
    }
  </style>
</head>
<body>
  <nav>
    <ul>
      <!-- Updated links with hash-based routing -->
      <li><a href="#/" class="tab-link" data-tab="tab_1">Home</a></li>
      <li><a href="#/projects" class="tab-link" data-tab="tab_2">Projects</a></li>
      <li><a href="#/publications" class="tab-link" data-tab="tab_3">Publications</a></li>
      <li><a href="#/stories" class="tab-link" data-tab="tab_4">Data Science Stories</a></li>
    </ul>
  </nav>

  <main>
    <!-- Home Section -->
    <section class="tabs__tab active" id="tab_1">
      <div id="home-section">
        <img src="https://github.com/bolleddu15/pradeepb-blog/blob/main/my%20image.jpeg?raw=true" alt="Pradeep Bolleddu" />
        <div class="welcome-text">
          <h1>Welcome</h1>
          <p>
            Welcome to My Data Science Journey!<br />
            Step into the world of data, where curiosity meets innovation! This is my personal blog, a space where I unravel the fascinating realms of data science, machine learning, and artificial intelligence.
            Here, I share my experiences, breakthroughs, and the insights I've gained while exploring the intersection of cutting-edge technology and real-world problem-solving.
            Join me as I dive deep into data-driven solutions that drive change, enhance automation, and push the boundaries of what's possible!
          </p>
        </div>
      </div>
    </section>

    <!-- Projects Section -->
<section class="tabs__tab" id="tab_2">
  <h1>Projects</h1>
  <ul id="projects-list">
    <!-- Project titles aligned left with updated styling -->
    <li>
      <a href="#/projects/predictive-maintenance" class="project-link" data-project="project1">
        Predictive Maintenance
      </a>
    </li>
    <li>
      <a href="#/projects/customer-segmentation" class="project-link" data-project="project2">
        Customer Segmentation
      </a>
    </li>
    <li>
      <a href="#/projects/academic-chatbot" class="project-link" data-project="project3">
        Academic Chatbot
      </a>
    </li>
    <li>
      <a href="#/projects/healthcare-classification" class="project-link" data-project="project4">
        Healthcare Image Classification
      </a>
    </li>
  </ul>
</section>

<!-- Project Details Section -->
<section class="tabs__tab" id="tab_project">
  <div id="project-content">
    <!-- Healthcare Image Classification (Project 4) Detailed Description -->
    <div id="project4-description" class="project-description">
      <h2>Healthcare Image Classification: Lung Cancer Detection using Chest X-rays</h2>
      <p>
        <strong>Problem Statement:</strong><br>
        In the realm of medical diagnostics, particularly oncology, the early detection of lung cancer is paramount to improving survival rates. However, traditional diagnostic methods, heavily reliant on radiologists, often face limitations in accuracy and scalability. This is particularly evident in resource-constrained environments where radiologists are overburdened, and time is a critical factor. The challenge lies not just in diagnosis, but in ensuring that the process is both accurate and efficient. The goal of this project was to build an AI-powered system capable of detecting lung cancer at early stages using chest X-rays, thus enhancing diagnostic speed, reliability, and ultimately, saving lives.
      </p>
      <p>
        <strong>Methodology & Technical Framework:</strong><br>
        To address this challenge, we turned to deep learning, specifically Convolutional Neural Networks (CNNs), which have been proven to be highly effective for image classification tasks. Given the complexity of medical imaging, a deep learning model was trained to identify patterns within chest X-rays that would be difficult for the human eye to detect, yet are critical in the early diagnosis of cancer. A key aspect of our approach was leveraging transfer learning. We chose ResNet-50, a pre-trained deep learning architecture, to benefit from its well-established ability to generalize from large-scale image datasets, such as ImageNet. By fine-tuning ResNet-50 on our specialized medical dataset, we could enhance its ability to recognize subtle anomalies in lung X-rays that indicate the presence of tumors.
      </p>
      <p>
        Additionally, we utilized image augmentation techniques such as rotation, scaling, and flipping to artificially increase the diversity of our training data, addressing the issue of limited labeled medical data. This step was crucial in ensuring our model was robust and could handle various imaging conditions.
      </p>
      <p>
        <strong>Project Goals & Objectives:</strong><br>
        The overarching goal of the project was to develop a deep learning model capable of detecting lung cancer in chest X-rays with a higher degree of accuracy than current manual methods. We set the following objectives:
        <ul>
          <li>Achieve a model accuracy of over 90% in detecting cancerous nodules.</li>
          <li>Minimize false positives and false negatives, ensuring that both diagnostic speed and reliability were optimized.</li>
          <li>Ensure the model could be deployed efficiently in real-time clinical settings, offering support to radiologists without disrupting existing workflows.</li>
        </ul>
      </p>
      <p>
        <strong>End-to-End Process:</strong><br>
        <em>Data Collection & Preprocessing:</em> The dataset consisted of over 50,000 labeled chest X-ray images sourced from multiple hospitals, ensuring diversity in terms of patient demographics and imaging conditions. The data was resized and normalized for model training.<br>
        <em>Model Selection & Training:</em> ResNet-50 was chosen for its deep residual learning capabilities. The model was trained with the Adam optimizer, with early stopping to prevent overfitting.<br>
        <em>Hyperparameter Tuning & Model Evaluation:</em> A grid search method and cross-validation were used for tuning hyperparameters, and the final model was evaluated using accuracy, precision, recall, and F1-score.<br>
        <em>Deployment & Integration:</em> The model was deployed on a cloud platform using TensorFlow Serving, integrated with a web-based front end for real-time clinical use via an API.
      </p>
      <p>
        <strong>Solutions and Achievements:</strong><br>
        The final model exceeded expectations, achieving a diagnostic accuracy of 92%, surpassing the initial goal of 90%. It demonstrated a significant reduction in diagnostic time, processing X-ray images in under a minute compared to the average 10-15 minutes required by radiologists. Moreover, the model successfully minimized false positives and false negatives, with precision and recall rates of 91% and 93%, respectively. In clinical tests, the model was able to assist radiologists in identifying lung cancer in earlier stages, offering support in diagnosing hard-to-spot tumors that could easily be overlooked. The deployment of the model resulted in a 30% reduction in the diagnostic turnaround time, allowing for faster decision-making in patient care. The integration of the AI model into the clinical workflow was seamless, and initial feedback from healthcare professionals indicated high levels of trust in the tool, particularly in terms of enhancing diagnostic confidence. This was achieved while maintaining full interpretability of the model’s decisions, as the Grad-CAM technique was used to visualize the regions of the X-ray the model focused on during its predictions.
      </p>
      <p>
        <strong>Results & Impact:</strong><br>
        Accuracy: 92%<br>
        Precision: 91%<br>
        Recall: 93%<br>
        F1-Score: 92%<br>
        Diagnostic Time Reduction: 30%<br>
        Model Integration: Seamless with existing hospital infrastructure, providing real-time predictions.
      </p>
      <p>
        This project has proven the potential of AI to revolutionize medical diagnostics. By implementing AI-driven decision support tools, we can not only increase the accuracy and efficiency of cancer detection but also significantly reduce the strain on overworked medical professionals, providing them with tools to make faster, more accurate decisions.
      </p>
      <p>
        <strong>Lessons Learned & Future Scope:</strong><br>
        While the project was a success, there were key lessons learned. Data quality remains a critical challenge in medical imaging, and while our model performed well on the dataset used, future work will involve expanding the dataset to include more diverse imaging conditions and patients from different geographical regions. Additionally, exploring multi-modal data (combining imaging with patient history) could enhance diagnostic precision even further. Looking ahead, the next phase will focus on scaling the model to include detection for other types of cancers and improving the model’s robustness in real-world, non-ideal conditions, such as images taken with low-quality equipment.
      </p>
    </div>
  </div>
</section>


    <!-- Publications Section -->
    <section class="tabs__tab" id="tab_3">
      <h1>Publications</h1>
      <ul id="publications-list">
        <li><a href="#/publications/deep-learning-healthcare" class="publication-link" data-publication="pub1">Deep Learning for Healthcare</a></li>
        <li><a href="#/publications/ai-financial-markets" class="publication-link" data-publication="pub2">AI in Financial Markets</a></li>
        <li><a href="#/publications/autonomous-systems" class="publication-link" data-publication="pub3">Autonomous Systems Research</a></li>
        <li><a href="#/publications/nlp-education" class="publication-link" data-publication="pub4">NLP in Education</a></li>
      </ul>
    </section>

    <!-- Publication Details Section -->
    <section class="tabs__tab" id="tab_publication">
      <div id="publication-content"></div>
    </section>

    <!-- Data Science Stories Section -->
    <section class="tabs__tab" id="tab_4">
      <h1>Data Science Stories</h1>
      <p>Real-world insights on how data science transforms industries.</p>
    </section>
  </main>

  <div class="footer">
    <a href="https://www.linkedin.com/in/pradeep-pb-bolleddu-4745991b2/" target="_blank" aria-label="LinkedIn">
      <img src="https://cdn-icons-png.flaticon.com/512/174/174857.png" alt="LinkedIn" />
    </a>
    <!-- Resume icon added after LinkedIn -->
    <a href="https://drive.google.com/drive/u/2/home" target="_blank" aria-label="Resume">
      <img src="https://cdn-icons-png.flaticon.com/512/942/942748.png" alt="Resume" />
    </a>
    <a href="https://github.com/bolleddu15" target="_blank" aria-label="GitHub">
      <img src="https://cdn-icons-png.flaticon.com/512/25/25231.png" alt="GitHub" />
    </a>
    <a href="mailto:bolleddu.pradeep01@gmail.com" aria-label="Email">
      <img src="https://cdn-icons-png.flaticon.com/512/732/732200.png" alt="Email" />
    </a>
  </div>

  <script>
    document.addEventListener("DOMContentLoaded", () => {
      const tabContents = document.querySelectorAll(".tabs__tab");
      const navLinks = document.querySelectorAll("nav ul li a");
      const projectContent = document.getElementById("project-content");
      const publicationContent = document.getElementById("publication-content");

      // Function to remove active class and set the active tab
      function setActiveTab(tabId) {
        tabContents.forEach(tab => tab.classList.remove("active"));
        const targetTab = document.getElementById(tabId);
        if (targetTab) {
          targetTab.classList.add("active");
        }
        // Update active class on navigation links (only for main tabs)
        navLinks.forEach(link => {
          const dataTab = link.getAttribute("data-tab");
          if (dataTab === tabId) {
            link.classList.add("active");
          } else {
            link.classList.remove("active");
          }
        });
      }

   // Function to remove active class and set the active tab
      function setActiveTab(tabId) {
        tabContents.forEach(tab => tab.classList.remove("active"));
        const targetTab = document.getElementById(tabId);
        if (targetTab) {
          targetTab.classList.add("active");
        }
        // Update active class on navigation links (only for main tabs)
        navLinks.forEach(link => {
          const dataTab = link.getAttribute("data-tab");
          if (dataTab === tabId) {
            link.classList.add("active");
          } else {
            link.classList.remove("active");
          }
        });
      }

// Function to load project details
function loadProject(projectId) {
  const projectData = {
    "project1": "<h2>Predictive Maintenance</h2>" +
                "<p class='project-description'>In the realm of industrial operations, the reliability of machinery and equipment is crucial for optimal performance and cost-efficiency. Imagine a world where you don’t have to wait for machines to break down before fixing them—where maintenance is predicted ahead of time, saving businesses time and money. This is the power of Predictive Maintenance.</p>" +
                "<p class='project-description'>The problem with traditional maintenance practices is that they are reactive. Machines are either serviced at scheduled intervals (time-based maintenance) or after they break down (failure-based maintenance). Both approaches are inefficient. Time-based maintenance may result in unnecessary repairs, while failure-based maintenance leads to downtime and unexpected costs. The problem is clear: businesses need a way to predict machine failures before they happen.</p>" +
                "<p class='project-description'>Predictive Maintenance uses machine learning algorithms to forecast when equipment will fail, based on historical data such as sensor readings, machine performance, and environmental conditions. The goal is to move away from reactive maintenance to a condition-based strategy, where maintenance happens based on actual data, not just time or failure.</p>" +
                "<p class='project-description'>In this project, we applied machine learning models like Random Forests, Support Vector Machines, and Gradient Boosting to predict when a particular asset would require maintenance. We used a dataset containing sensor data from machines, including readings like temperature, pressure, and vibrations.</p>" +
                "<h3>Data Preprocessing and Feature Engineering</h3>" +
                "<p class='project-description'>The first step was to clean the data and handle missing values. Then, feature engineering played a vital role in extracting meaningful patterns from the raw data. Statistical features like mean, standard deviation, and moving averages were calculated over time windows to summarize the machine’s behavior.</p>" +
                "<h3>Model Building</h3>" +
                "<p class='project-description'>For predictive maintenance, it was essential to treat this as a classification problem—we wanted to predict if a machine was likely to fail within a specific time frame. Models were trained to categorize machines into either “maintenance required” or “maintenance not required” classes.</p>" +
                "<p class='project-description'>The models used were:</p>" +
                "<ul>" +
                "<li><b>Random Forests</b>: We leveraged this model for its robustness against overfitting and its ability to handle large, complex datasets.</li>" +
                "<li><b>Support Vector Machines (SVM)</b>: For clearer decision boundaries between classes.</li>" +
                "<li><b>Gradient Boosting</b>: To boost performance by correcting the mistakes of previous models in a sequential manner.</li>" +
                "</ul>" +
                "<h3>Formula for Predictive Maintenance</h3>" +
                "<p class='project-description'>To quantify the time to failure, we used the survival analysis model and logistic regression for predicting failure probabilities. The basic formula behind logistic regression is:</p>" +
                "<p class='project-description'>P(failure) = 1 / (1 + e<sup>-(β<sub>0</sub> + β<sub>1</sub>⋅X<sub>1</sub> + β<sub>2</sub>⋅X<sub>2</sub> + … + β<sub>n</sub>⋅X<sub>n</sub>)</sup></p>" +
                "<p class='project-description'>Where:</p>" +
                "<ul>" +
                "<li><b>P(failure)</b> is the probability of failure within the given time window.</li>" +
                "<li><b>β<sub>0</sub>, β<sub>1</sub>, …, β<sub>n</sub></b> are the model coefficients.</li>" +
                "<li><b>X<sub>1</sub>, X<sub>2</sub>, …, X<sub>n</sub></b> are the features derived from the sensor data.</li>" +
                "</ul>" +
                "<h3>Results and Findings</h3>" +
                "<p class='project-description'>After training the models, we evaluated them using metrics such as accuracy, precision, recall, and F1 score. The models provided valuable insights into which machines required immediate attention and predicted failure windows. By shifting to predictive maintenance, businesses saw:</p>" +
                "<ul>" +
                "<li>Reduction in downtime by up to 30%.</li>" +
                "<li>Cost savings by performing maintenance only when necessary.</li>" +
                "<li>Improved asset lifespan due to timely interventions.</li>" +
                "</ul>" +
                "<h3>Challenges</h3>" +
                "<p class='project-description'>Predictive maintenance projects are not without their challenges. The most significant hurdle is data quality—sensor data can be noisy or incomplete, which directly impacts the accuracy of the model. Moreover, selecting the right features from the available data to ensure model interpretability remains an ongoing struggle.</p>" +
                "<h3>Conclusion</h3>" +
                "<p class='project-description'>By leveraging machine learning, Predictive Maintenance transforms how businesses approach machinery upkeep. Instead of waiting for machines to break down, the future lies in anticipating problems before they occur. It’s a shift that not only enhances operational efficiency but also reduces costs and extends the lifespan of equipment.</p>",

    "project2": "<h2>Customer Segmentation</h2>" +
                "<p class='project-description'>In today's hyper-competitive business world, understanding your customers is no longer a luxury—it’s a necessity. With so many companies vying for attention, the key to success lies in identifying and understanding different customer segments. Customer Segmentation helps businesses categorize their customers based on shared characteristics, which allows for targeted marketing, personalized experiences, and improved customer retention.</p>" +
                "<p class='project-description'>Imagine a company that sends the same marketing email to all its customers. For some, this may resonate, but for many others, it’s irrelevant. A one-size-fits-all marketing strategy is ineffective, and the company risks losing customers to competitors who offer more personalized experiences. The problem is clear: businesses need to segment their customers intelligently.</p>" +
                "<p class='project-description'>In this project, we used unsupervised learning techniques, specifically K-means clustering, to identify distinct customer groups based on demographic data, purchasing behavior, and browsing habits. The ultimate goal was to deliver targeted marketing strategies based on segment characteristics.</p>" +
                "<h3>Data Preprocessing</h3>" +
                "<p class='project-description'>The first task was cleaning and preparing the data. We focused on customer attributes such as age, income, purchase history, and geographical location. Standardizing these features was crucial for the K-means algorithm to function effectively.</p>" +
                "<h3>Model Building</h3>" +
                "<p class='project-description'>K-means clustering is one of the most commonly used techniques for customer segmentation. The core idea behind K-means is to partition data into K distinct clusters. The algorithm assigns each data point to the cluster whose center is closest to the data point.</p>" +
                "<ul>" +
                "<li>Step 1: Initialize K centroids (randomly chosen).</li>" +
                "<li>Step 2: Assign each data point to the nearest centroid.</li>" +
                "<li>Step 3: Recalculate the centroids based on the assigned data points.</li>" +
                "<li>Step 4: Repeat steps 2 and 3 until convergence.</li>" +
                "</ul>" +
                "<h3>Formula for K-means</h3>" +
                "<p class='project-description'>The objective of K-means is to minimize the sum of squared distances between data points and their corresponding centroids:</p>" +
                "<p class='project-description'>J = Σ<sub>i=1</sub><sup>K</sup> Σ<sub>x<sub>j</sub> ∈ C<sub>i</sub></sub> || x<sub>j</sub> - μ<sub>i</sub> ||<sup>2</sup></p>" +
                "<p class='project-description'>Where:</p>" +
                "<ul>" +
                "<li><b>J</b> is the objective function (sum of squared distances).</li>" +
                "<li><b>K</b> is the number of clusters.</li>" +
                "<li><b>C<sub>i</sub></b> represents the points assigned to cluster i.</li>" +
                "<li><b>μ<sub>i</sub></b> is the centroid of cluster i.</li>" +
                "<li><b>x<sub>j</sub></b> is a data point.</li>" +
                "</ul>" +
                "<h3>Results and Findings</h3>" +
                "<p class='project-description'>After applying K-means clustering, we identified several customer segments based on purchasing behavior and demographic profiles. These segments enabled the business to:</p>" +
                "<ul>" +
                "<li>Target specific groups with tailored marketing campaigns, increasing engagement by 25%.</li>" +
                "<li>Personalize product recommendations, resulting in a 15% increase in sales.</li>" +
                "<li>Improve customer retention by identifying at-risk groups and offering them incentives.</li>" +
                "</ul>" +
                "<h3>Challenges</h3>" +
                "<p class='project-description'>The primary challenge was choosing the optimal number of clusters (K). The elbow method helped in this process, but in practice, customer segments can sometimes overlap, making it difficult to draw clear boundaries.</p>" +
                "<h3>Conclusion</h3>" +
                "<p class='project-description'>Customer Segmentation allows businesses to make smarter decisions, driving revenue and customer loyalty. By understanding the unique characteristics of each segment, companies can offer targeted products, personalized marketing, and enhance customer experiences.</p>",
      "project3": `<h2>Academic Chatbot</h2>
                  <p class='project-description'>
                    The Academic Chatbot is engineered to serve as an intelligent assistant for academic institutions. By merging Retrieval-Augmented Generation (RAG) with Large Language Models (LLMs), the chatbot not only understands complex natural language queries but also dynamically retrieves relevant academic content to produce contextually enriched responses.
                    <br><br>
                    <strong>Technical Components:</strong><br>
                    <strong>Retrieval-Augmented Generation (RAG):</strong> RAG is an innovative approach that synergizes retrieval mechanisms with generative models. It enables the system to reference external documents and databases during response generation, thereby enhancing factual correctness and context-awareness.<br>
                    <strong>Retriever Module:</strong> Given a query <i>q</i>, the retriever identifies a subset of relevant documents <i>Dq</i> from a large corpus <i>D</i>. The retriever often employs vector representations and similarity search algorithms (e.g., cosine similarity over embeddings) to rank documents.<br>
                    <strong>Generator Module:</strong> The generator conditions on both the query <i>q</i> and the retrieved documents <i>Dq</i> to produce a coherent answer <i>r</i>. This process uses deep sequence-to-sequence models (such as GPT or T5) fine-tuned on domain-specific data.<br>
                    <strong>Large Language Models (LLMs):</strong> LLMs are pre-trained on vast datasets and are capable of capturing semantic nuances and complex linguistic structures. In this project, LLMs provide the backbone for natural language understanding and generation. Their capability to generalize over diverse contexts is augmented by the retrieval component.<br>
                    <strong>Mathematical Formulation:</strong> The end-to-end operation of the RAG system can be seen as an optimization problem where we aim to maximize the likelihood of generating a correct response <i>r</i> given a query <i>q</i>.<br><br>
                    <strong>Implementation Details:</strong><br>
                    <strong>Data Sources:</strong> The system integrates academic databases, course catalogs, research papers, and FAQs to build a comprehensive knowledge base.<br>
                    <strong>Frameworks & Libraries:</strong> Python, Hugging Face Transformers, Faiss.<br>
                    <strong>Training Strategy:</strong> Pre-training on large datasets followed by fine-tuning using domain-specific academic content.<br><br>
                    <strong>Expert Considerations:</strong><br>
                    <strong>Scalability:</strong> The system is designed for scalability, with modular components that can be independently updated as new academic content becomes available.<br>
                    <strong>Evaluation Metrics:</strong> BLEU, ROUGE, and human-in-the-loop feedback.
                  </p>`,
    "project4": `<h2>Healthcare Image Classification</h2>
                  <video controls width="600">
                    <source src="https://github.com/bolleddu15/pradeepb-blog/blob/main/neural%20networks%20video.mp4?raw=true" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                  <p class='project-description'>
                    The Healthcare Image Classification project aims to assist radiologists by automating the detection and classification of diseases from medical images. Primarily focusing on chest X-rays, the system utilizes Convolutional Neural Networks (CNNs) to identify patterns that indicate conditions such as lung cancer and pneumonia.
                    <br><br>
                    <strong>Technical Components:</strong><br>
                    <strong>Image Classification Using CNNs:</strong> CNNs are the cornerstone of modern image analysis due to their ability to learn spatial hierarchies through convolutional filters. The architecture typically involves several layers:<br>
                    <strong>Convolutional Layers:</strong> These layers apply learnable filters to extract local features.<br>
                    <strong>Pooling Layers:</strong> Pooling reduces the spatial dimensions, retaining essential information while minimizing computational load.<br>
                    <strong>Fully Connected Layers:</strong> These layers integrate features extracted by the convolutional layers to produce a final classification output.<br>
                    <strong>Loss Function and Optimization:</strong> Cross-entropy loss is employed for multi-class classification.<br><br>
                    <strong>Implementation Details:</strong><br>
                    <strong>Data Pipeline:</strong> Curated dataset of chest X-rays with annotations, using data augmentation techniques.<br>
                    <strong>Frameworks & Libraries:</strong> TensorFlow, PyTorch, Keras, OpenCV.<br>
                    <strong>Training Strategy:</strong> Cross-validation and test set evaluation with regularization techniques such as dropout and batch normalization.<br><br>
                    <strong>Expert Considerations:</strong><br>
                    <strong>Interpretability:</strong> Grad-CAM is integrated to visualize model attention.<br>
                    <strong>Deployment:</strong> Designed for integration into hospital systems, ensuring performance and regulatory standards.<br>
                    <strong>Performance Metrics:</strong> Accuracy, precision, recall, F1-score, and AUC.
                  </p>`
  };
  projectContent.innerHTML = projectData[projectId] || "<p>Project not found.</p>";
  setActiveTab("tab_project");
}
  


      // Function to load publication details
      function loadPublication(publicationId) {
        const publicationData = {
          "pub1": "<h2>Deep Learning for Healthcare</h2><p class='project-description'>This research explores the use of deep learning models in predicting health outcomes.</p>",
          "pub2": "<h2>AI in Financial Markets</h2><p class='project-description'>Application of AI algorithms in stock price prediction and financial decision-making.</p>",
          "pub3": "<h2>Autonomous Systems Research</h2><p class='project-description'>Investigating the latest advancements in autonomous systems and robotics.</p>",
          "pub4": "<h2>NLP in Education</h2><p class='project-description'>Exploring how Natural Language Processing can revolutionize educational tools.</p>"
        };
        publicationContent.innerHTML = publicationData[publicationId] || "<p>Publication not found.</p>";
        setActiveTab("tab_publication");
      }

      // Listen for clicks on main navigation links
      document.querySelectorAll(".tab-link").forEach(link => {
        link.addEventListener("click", event => {
          event.preventDefault();
          window.location.hash = link.getAttribute("href");
        });
      });

      // Listen for clicks on project links
      document.querySelectorAll(".project-link").forEach(link => {
        link.addEventListener("click", event => {
          event.preventDefault();
          window.location.hash = link.getAttribute("href");
        });
      });

      // Listen for clicks on publication links
      document.querySelectorAll(".publication-link").forEach(link => {
        link.addEventListener("click", event => {
          event.preventDefault();
          window.location.hash = link.getAttribute("href");
        });
      });

      // Function to handle hash-based routing
      function handleRouting() {
        const hash = window.location.hash || "#/";
        if (hash === "#/" || hash === "#") {
          setActiveTab("tab_1");
          return;
        }
        if (hash.startsWith("#/projects")) {
          if (hash.includes("predictive-maintenance")) {
            loadProject("project1");
          } else if (hash.includes("customer-segmentation")) {
            loadProject("project2");
          } else if (hash.includes("academic-chatbot")) {
            loadProject("project3");
          } else if (hash.includes("healthcare-classification")) {
            loadProject("project4");
          } else {
            setActiveTab("tab_2");
          }
        } else if (hash.startsWith("#/publications")) {
          if (hash.includes("deep-learning-healthcare")) {
            loadPublication("pub1");
          } else if (hash.includes("ai-financial-markets")) {
            loadPublication("pub2");
          } else if (hash.includes("autonomous-systems")) {
            loadPublication("pub3");
          } else if (hash.includes("nlp-education")) {
            loadPublication("pub4");
          } else {
            setActiveTab("tab_3");
          }
        } else if (hash.startsWith("#/stories")) {
          setActiveTab("tab_4");
        } else {
          setActiveTab("tab_1");
        }
      }

      window.addEventListener("hashchange", handleRouting);
      handleRouting();
    });
  </script>
</body>
</html>
